{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.use('Agg') # Must be before importing matplotlib.pyplot or pylab!\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.insert(0,'/local2/home/tong/caffe-master/python')\n",
    "import caffe\n",
    "\n",
    "import os\n",
    "from matplotlib import rc\n",
    "rc('mathtext', default='regular')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "caffe.set_mode_gpu()\n",
    "caffe.set_device(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for training\n",
    "solver = caffe.SGDSolver('/local2/home/tong/fashionRecommendation/models/fashionNet_3/fashion_solver_3.prototxt')\n",
    "#CHANGE!!!\n",
    "solver.net.copy_from('/local2/home/tong/fashionRecommendation/models/VGG_M_2048/VGG_CNN_M_2048.caffemodel')\n",
    "#CHANGE!!!\n",
    "solver.test_nets[0].copy_from('/local2/home/tong/fashionRecommendation/models/VGG_M_2048/VGG_CNN_M_2048.caffemodel')\n",
    "#solver.net.copy_from('/local2/home/tong/fashionRecommendation/models/fashionNet_3/training_record/s_fashion_params_3_58315.caffemodel')\n",
    "# solver.test_nets[0].copy_from('/local2/home/tong/fashionRecommendation/models/fashionNet_3/training_record/s_fashion_params_3_58315.caffemodel')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for saving caffemodel\n",
    "net = caffe.Net('/local2/home/tong/fashionRecommendation/models/fashionNet_3/fashion_deploy_3.prototxt', caffe.TEST)\n",
    "#net = caffe.Net('/local2/home/tong/fashionRecommendation/models/VGG_M_2048/VGG_CNN_M_2048_deploy.prototxt', caffe.TEST)\n",
    "#net = caffe.Net('/home/tonghe2/fashionRecommendation/models/fashionNet_3/fashion_deploy_3.prototxt', caffe.TEST)\n",
    "#net = caffe.Net('/home/tonghe2/fashionRecommendation/models/VGG_M_2048/VGG_CNN_M_2048_deploy.prototxt', caffe.TEST)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #verify solver.net & solver.test_nets[0]\n",
    "# solver.net.forward()\n",
    "# solver.test_nets[0].forward()\n",
    "\n",
    "# print(solver.net.blobs['accuracy'].data, \n",
    "#       solver.test_nets[0].blobs['accuracy'].data)\n",
    "\n",
    "# if solver.test_nets[0].blobs['accuracy'].data > 0.3:\n",
    "#     print 'feasible!'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_avg(test_iter):\n",
    "    avg_accu = 0\n",
    "    avg_loss = 0\n",
    "    y_l_true = []\n",
    "    y_l_pred = []\n",
    "    y_r_true = []\n",
    "    y_r_pred = []\n",
    "    dd_dislike = 0\n",
    "    dd_count = 0\n",
    "    dl_dislike = 0\n",
    "    dl_count = 0\n",
    "    ld_like = 0\n",
    "    ld_count = 0\n",
    "    ll_like = 0\n",
    "    ll_count = 0\n",
    "    nr_tuples_pos = []\n",
    "    count_posi = 0\n",
    "    nr_tuples_neg = []\n",
    "    count_nega = 0\n",
    "    scores_pos = []\n",
    "    scores_neg = []\n",
    "    for i in range(0,test_iter):\n",
    "        solver.test_nets[0].forward()\n",
    "        avg_accu = avg_accu + solver.test_nets[0].blobs['accuracy'].data\n",
    "        avg_loss = avg_loss + solver.test_nets[0].blobs['rank_Loss'].data\n",
    "        for j in range(0,len(solver.test_nets[0].blobs['label_top'].data)):\n",
    "            y_l_pred.append(0 if solver.test_nets[0].blobs['metric_fc3'].data[j][0]>solver.test_nets[0].blobs['metric_fc3'].data[j][1] else 1)\n",
    "            y_l_true.append(1)\n",
    "            # count posi tuple number\n",
    "            count_posi = count_posi + 1\n",
    "            # record pre_score for this posi outfit\n",
    "            scores_pos.append(solver.test_nets[0].blobs['metric_fc3_softmax'].data[j][1])\n",
    "            y_r_pred.append(0 if solver.test_nets[0].blobs['metric_fc3_n'].data[j][0]>solver.test_nets[0].blobs['metric_fc3_n'].data[j][1] else 1)\n",
    "            y_r_true.append(0)\n",
    "            # count neg tuple number\n",
    "            count_nega = count_nega + 1\n",
    "            # record pre_score for this nega outfit\n",
    "            scores_neg.append(solver.test_nets[0].blobs['metric_fc3_softmax_n'].data[j][1])\n",
    "            #dd\n",
    "            if (solver.test_nets[0].blobs['metric_fc3_n'].data[j][0]>solver.test_nets[0].blobs['metric_fc3_n'].data[j][1]):\n",
    "                dd_count = dd_count + 1\n",
    "                dd_dislike = dd_dislike + solver.test_nets[0].blobs['metric_fc3_softmax_n'].data[j][0]\n",
    "            #dl\n",
    "            else:\n",
    "                dl_count = dl_count + 1\n",
    "                dl_dislike = dl_dislike + solver.test_nets[0].blobs['metric_fc3_softmax_n'].data[j][0]\n",
    "            #ld\n",
    "            if (solver.test_nets[0].blobs['metric_fc3'].data[j][0]>solver.test_nets[0].blobs['metric_fc3'].data[j][1]):\n",
    "                ld_count = ld_count + 1\n",
    "                ld_like = ld_like + solver.test_nets[0].blobs['metric_fc3_softmax'].data[j][1]\n",
    "            #ll\n",
    "            else:\n",
    "                ll_count = ll_count + 1\n",
    "                ll_like = ll_like + solver.test_nets[0].blobs['metric_fc3_softmax'].data[j][1]\n",
    "    cMat_l = confusion_matrix(y_l_true, y_l_pred)\n",
    "    if(len(cMat_l)==1):\n",
    "        cMat_l = [[0,0],[0,cMat_l[0][0]]]\n",
    "    cMat_r = confusion_matrix(y_r_true, y_r_pred)\n",
    "    if(len(cMat_r)==1):\n",
    "        cMat_r = [[cMat_r[0][0],0],[0,0]]\n",
    "    nr_tuples_pos.append(count_posi)\n",
    "    nr_tuples_neg.append(count_nega)\n",
    "    return (avg_accu/test_iter), (avg_loss/test_iter),cMat_l,cMat_r, \\\n",
    "           (0 if dd_count==0 else dd_dislike/dd_count), (0 if dl_count==0 else dl_dislike/dl_count), ((dd_dislike+dl_dislike)/(dd_count+dl_count)), \\\n",
    "           (0 if ld_count==0 else ld_like/ld_count), (0 if ll_count==0 else ll_like/ll_count), ((ld_like+ll_like)/(ld_count+ll_count)), \\\n",
    "           scores_pos, scores_neg, nr_tuples_pos, nr_tuples_neg        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_ndcg(scores_pos, scores_neg, nr_tuples_pos, nr_tuples_neg,\n",
    "\t\t\t fn_out='', tuples_pos=None, tuples_neg=None, nr_return=0):\n",
    "\t\"\"\"\n",
    "\t- nr_tuples_pos[ui]: the number of positive outfits for user ui\n",
    "\t- nr_tuples_neg[ui]: the number of negative(neutral) outfits for user ui\n",
    "\t\n",
    "\t- scores_pos: scores for positive outfits. \n",
    "\t\t\t\t  scores_pos[0:nr_tuples_pos[0]] are the scores for the first user.\n",
    "\t\t\t\t  scores_pos[nr_tuples_pos[0]:nr_tuples_pos[0]+nr_tuples_pos[1]] are the scores for the second user.\n",
    "\t\t\t\t\n",
    "\t- scores_neg: scores for negative outfits. \n",
    "\t\t\t\t  scores_neg[0:nr_tuples_neg[0]] are the scores for the first user.\n",
    "\t\t\t\t  scores_neg[nr_tuples_neg[0]:nr_tuples_neg[0]+nr_tuples_neg[1]] are the scores for the second user.\n",
    "\t\"\"\"\n",
    "\tif fn_out != '':\n",
    "            fid_out = open(fn_out, 'w')\n",
    "\n",
    "\tm = 10 # pre-determined ndcg size\n",
    "\tndcg_ct = np.zeros(m)\n",
    "\tndcg_at = np.zeros(m)\n",
    "\tmean_ndcg = 0\n",
    "\ts_ind_pos = 0 # update posi outfit index for each user\n",
    "\ts_ind_neg = 0 # update neutral outfit index for each user\n",
    "\tnr_users = len(nr_tuples_pos) # user number\n",
    "\tfor ui in range(nr_users): # for each user\n",
    "\t\tcount_q = nr_tuples_pos[ui] + nr_tuples_neg[ui] # total outfits (both posi & neutral) number of this user\n",
    "\t\tlabel = np.zeros(nr_tuples_pos[ui]+nr_tuples_neg[ui]) # labels for all outfits of this user, 1 for posi & 0 for neutral\n",
    "\t\tlabel[:nr_tuples_pos[ui]] = 1\n",
    "\t\ttarget = np.empty(nr_tuples_pos[ui]+nr_tuples_neg[ui]) # scores for all outfits (both posi & neutral)\n",
    "\t\ttarget[:nr_tuples_pos[ui]] = scores_pos[s_ind_pos:s_ind_pos+nr_tuples_pos[ui]] # scores for posi outfits\n",
    "\t\ttarget[nr_tuples_pos[ui]:] = scores_neg[s_ind_neg:s_ind_neg+nr_tuples_neg[ui]] # scores for neutral outfits\n",
    "\t\tif fn_out != '':\n",
    "\t\t\ttuples = np.hstack((tuples_pos[:,s_ind_pos:s_ind_pos+nr_tuples_pos[ui]],\n",
    "\t\t\t\t\t\t\t\ttuples_neg[:,s_ind_neg:s_ind_neg+nr_tuples_neg[ui]]))\n",
    "\t\ts_ind_pos += nr_tuples_pos[ui]\n",
    "\t\ts_ind_neg += nr_tuples_neg[ui]\n",
    "\n",
    "\t\tndcg_size = min(m, count_q) # actual ndcg size\n",
    "\t\tideal_dcg = np.empty(count_q) # for computing ideal ndcg value\n",
    "\t\tdcg = np.empty(count_q) # for computing dcg (without normalization by N_m yet)\n",
    "\t\tndcg = 0\n",
    "\t\torder = np.argsort(-label) # sort label in descending order, returns the sequential indices of label \n",
    "\t\tideal_dcg[0] = pow(2.0, label[order[0]]) - 1 # compute ideal_ndcg@m (m=1)\n",
    "\t\tfor i in range(1, count_q): # compute ideal_ndcg@m (m=2,...,M), M is total outfits number of this user\n",
    "\t\t\tideal_dcg[i] = ideal_dcg[i-1]+(pow(2.0, label[order[i]])\n",
    "\t\t\t\t\t\t\t\t\t\t   - 1)*np.log(2.0)/np.log(i+1.0)\n",
    "\t\torder = np.argsort(-target) # sort scores for all outfits in descending order, returns the indices\n",
    "\t\tdcg[0] = pow(2.0, label[order[0]]) - 1 # compute dcg@m (m=1)\n",
    "\t\tfor i in range(1, count_q): # compute dcg@m (m=2,...,M), M is total outfits number of this user\n",
    "\t\t\tdcg[i] = dcg[i-1]+(pow(2.0, label[order[i]])\n",
    "\t\t\t\t\t\t\t   - 1)*np.log(2.0)/np.log(i+1.0)\n",
    "\t\tif ideal_dcg[0] > 0: # at least there should be one posi outfit for this user, or else somehting is wrong here\n",
    "\t\t\tfor i in range(count_q): # for each @m (m=1,...,M)\n",
    "\t\t\t\tndcg += dcg[i] / ideal_dcg[i] # add up all ndcg@m (m=1,..,M) for this user\n",
    "\t\t\tfor i in range(ndcg_size): # compute top 10 ndcg for all users\n",
    "\t\t\t\tndcg_ct[i] += 1 # record outfits num at each place, among top 10\n",
    "\t\t\t\tndcg_at[i] += dcg[i] / ideal_dcg[i] # add up ndcg value at each place, among top 10\n",
    "\t\telse: # if we only have no posi outfit for this user\n",
    "\t\t\tndcg = 0 # ndcg is 0 for this user, because there is not point of ranking anymore for him/here\n",
    "\t\tm_ndcg = ndcg / count_q # mean ndcg for this user\n",
    "\t\tmean_ndcg += m_ndcg # add up mean ndcg for all users\n",
    "\n",
    "\t\tif fn_out != '':\n",
    "\t\t\tfid_out.write('%f\\n' % m_ndcg)\n",
    "\t\t\tn_out = min(count_q, nr_return)\n",
    "\t\t\tfor i in range(n_out):\n",
    "\t\t\t\tfid_out.write('%d ' % label[order[i]])\n",
    "\t\t\t\tfor jj in range(tuples.shape[0]):\n",
    "\t\t\t\t\tfid_out.write('%d ' % tuples[jj, order[i]])\n",
    "\t\t\t\tfid_out.write('\\n')\n",
    "\n",
    "\tmean_ndcg /= nr_users # mean ndcg for all users as a whole\n",
    "\tfor i in range(m): # top 10 mean ndcg for all users as a whole\n",
    "\t\tndcg_at[i] /= ndcg_ct[i]\n",
    "\n",
    "\tif fn_out != '':\n",
    "\t\tfid_out.close()\n",
    "\n",
    "\treturn (mean_ndcg, ndcg_at)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#training solver.net\n",
    "recordDir = '/local2/home/tong/fashionRecommendation/models/fashionNet_3/training_record/'\n",
    "\n",
    "one_twen_epoch = 184 # 1/150 train epoch\n",
    "test_interval = 184 # 1/150 train epoch\n",
    "\n",
    "test_iter = 180 # 1 test epoch\n",
    "\n",
    "k = 0\n",
    "start_iter = 0 #CHANGE!!!\n",
    "end_iter = 9178*3 #3 epoch CHANGE!!!\n",
    "\n",
    "params = net.params.keys()\n",
    "\n",
    "solver.net.forward() #CHANGE!!!\n",
    "\n",
    "train_accu = 0 #CHANGE!!!\n",
    "train_los = 0 #CHANGE!!!\n",
    "train_avg_accu = 0\n",
    "train_avg_loss = 0\n",
    "train_cur_accu = 0\n",
    "train_cur_loss = 0\n",
    "test_accu = 0\n",
    "test_loss = 0\n",
    "\n",
    "for i in range (start_iter,end_iter+1):\n",
    "# if (i != start_iter): #CHANGE!!!\n",
    "    # save train data\n",
    "    tr_avg_accu = open(recordDir + 'train_avg_accu.txt','a')\n",
    "    tr_avg_loss = open(recordDir + 'train_avg_loss.txt','a')\n",
    "    train_accu = train_accu + solver.net.blobs['accuracy'].data\n",
    "    train_los = train_los + solver.net.blobs['rank_Loss'].data\n",
    "    train_avg_accu = train_accu/(i+1)\n",
    "    train_avg_loss = train_los/(i+1)\n",
    "    tr_avg_accu.write(str(i)+' '+str(train_avg_accu)+'\\r\\n')\n",
    "    tr_avg_loss.write(str(i)+' '+str(train_avg_loss)+'\\r\\n')\n",
    "    tr_avg_accu.close()\n",
    "    tr_avg_loss.close()\n",
    "    if(i%5==0):\n",
    "        print(\"\\nIters done:{}, avg_accu={}, avg_loss={}.\\n\".format(i,train_avg_accu,train_avg_loss))\n",
    "    tr_accu = open(recordDir + 'train_accu.txt','a')\n",
    "    tr_loss = open(recordDir + 'train_loss.txt','a')\n",
    "    train_cur_accu = solver.net.blobs['accuracy'].data\n",
    "    train_cur_loss = solver.net.blobs['rank_Loss'].data\n",
    "    tr_accu.write(str(i)+' '+str(train_cur_accu)+'\\r\\n')\n",
    "    tr_loss.write(str(i)+' '+str(train_cur_loss)+'\\r\\n')\n",
    "    tr_accu.close()\n",
    "    tr_loss.close()\n",
    "    if(i%5==0):\n",
    "        print(\"              bat_accu={}, bat_loss={}.\\n\".format(train_cur_accu,train_cur_loss))\n",
    "    # validation, save caffemodel, visualization, and stop criteria\n",
    "    if (i%test_interval==0 or i==end_iter):\n",
    "        # validation\n",
    "        test_accu,test_loss,cMat_l,cMat_r,dd_dislike,dl_dislike,d_dislike,ld_like,ll_like,l_like=test_avg(test_iter)\n",
    "        mean_ndcg, ndcg_at = get_ndcg(scores_pos, scores_neg, nr_tuples_pos, nr_tuples_neg)\n",
    "        te_accu = open(recordDir + 'test_accu.txt','a')\n",
    "        te_loss = open(recordDir + 'test_loss.txt','a')\n",
    "        te_accu.write(str(i)+' '+str(test_accu)+'\\r\\n')\n",
    "        te_loss.write(str(i)+' '+str(test_loss)+'\\r\\n')\n",
    "        te_accu.close()\n",
    "        te_loss.close()\n",
    "        te_l_conf = open(recordDir + 'conf_l_matrix.txt','a')\n",
    "        te_l_conf.write(str(i)+' '+str(cMat_l[0][0])+' '+str(cMat_l[0][1])+' '+str(cMat_l[1][0])+' '+str(cMat_l[1][1])+'\\r\\n')\n",
    "        te_l_conf.close()\n",
    "        te_r_conf = open(recordDir + 'conf_r_matrix.txt','a')\n",
    "        te_r_conf.write(str(i)+' '+str(cMat_r[0][0])+' '+str(cMat_r[0][1])+' '+str(cMat_r[1][0])+' '+str(cMat_r[1][1])+'\\r\\n')\n",
    "        te_r_conf.close()\n",
    "        dd_f = open(recordDir + 'dd_dislike.txt','a')\n",
    "        dd_f.write(str(i)+' '+str(dd_dislike)+'\\r\\n')\n",
    "        dd_f.close()\n",
    "        dl_f = open(recordDir + 'dl_dislike.txt','a')\n",
    "        dl_f.write(str(i)+' '+str(dl_dislike)+'\\r\\n')\n",
    "        dl_f.close()\n",
    "        d_f = open(recordDir + 'd_dislike.txt','a')\n",
    "        d_f.write(str(i)+' '+str(d_dislike)+'\\r\\n')\n",
    "        d_f.close()\n",
    "        ld_f = open(recordDir + 'ld_like.txt','a')\n",
    "        ld_f.write(str(i)+' '+str(ld_like)+'\\r\\n')\n",
    "        ld_f.close()\n",
    "        ll_f = open(recordDir + 'll_like.txt','a')\n",
    "        ll_f.write(str(i)+' '+str(ll_like)+'\\r\\n')\n",
    "        ll_f.close()\n",
    "        l_f = open(recordDir + 'l_like.txt','a')\n",
    "        l_f.write(str(i)+' '+str(l_like)+'\\r\\n')\n",
    "        l_f.close()\n",
    "        # record mean_ndcg & ndcg_at\n",
    "        ndcg_f = open(recordDir+'ndcg.txt','a')\n",
    "        ndcg_f.write(str(i)+' '+str(mean_ndcg)+'\\r\\n')\n",
    "        for n in range(0,len(ndcg_at)):\n",
    "            ndcg_f.write(str(i)+' '+str(ndcg_at[n])+'\\r\\n')\n",
    "        ndcg_f.close()\n",
    "        print(\"\\nIters done:{}, VAL_accu={}, VAL_loss={}.\\n\".format(i,test_accu,test_loss))\n",
    "        # save caffemodel\n",
    "        source_params = {pr: (solver.net.params[pr][0].data,solver.net.params[pr][1].data) for pr in params}\n",
    "        target_params = {pr: (net.params[pr][0].data,net.params[pr][1].data) for pr in params}\n",
    "        for pr in params:\n",
    "            target_params[pr][0][...] = source_params[pr][0] #weights\n",
    "            target_params[pr][1][...] = source_params[pr][1] #bias\n",
    "        net.save(recordDir+'fashion_params_3_'+str(i)+'.caffemodel') \n",
    "        #visualization for discovering overfitting\n",
    "        train_accu_whole = []\n",
    "        train_loss_whole = []\n",
    "        test_accu_whole = []\n",
    "        test_loss_whole = []\n",
    "        train_accu_whole = open(recordDir+'train_avg_accu.txt').readlines() \n",
    "        train_loss_whole = open(recordDir+'train_avg_loss.txt').readlines()\n",
    "        test_accu_whole = open(recordDir+'test_accu.txt').readlines()\n",
    "        test_loss_whole = open(recordDir+'test_loss.txt').readlines()\n",
    "        train_iter_idx = []\n",
    "        train_loss = []\n",
    "        train_accuracy = []\n",
    "        test_iter_idx = []\n",
    "        test_loss = []\n",
    "        test_accuracy = []\n",
    "        for i in range(0, len(train_accu_whole)):\n",
    "#             if i % test_interval == 0:\n",
    "            #train_iter_idx\n",
    "            train_iter_idx.append(int(train_accu_whole[i].strip('\\r\\n').split(' ')[0]))\n",
    "            #train_loss\n",
    "            train_loss.append(float(train_loss_whole[i].strip('\\r\\n').split(' ')[1]))\n",
    "            #train_accuracy\n",
    "            train_accuracy.append(float(train_accu_whole[i].strip('\\r\\n').split(' ')[1]))\n",
    "        for i in range(0, len(test_accu_whole)):\n",
    "            #test_iter_idx\n",
    "            test_iter_idx.append(int(test_accu_whole[i].strip('\\r\\n').split(' ')[0]))\n",
    "            #test_loss\n",
    "            test_loss.append(float(test_loss_whole[i].strip('\\r\\n').split(' ')[1]))\n",
    "            #test_accuracy\n",
    "            test_accuracy.append(float(test_accu_whole[i].strip('\\r\\n').split(' ')[1]))\n",
    "        fig = plt.figure()\n",
    "        ax_left = fig.add_subplot(111)\n",
    "        ax_left.plot(train_iter_idx, train_loss, '--rp', label = 'Avg_T_Loss')\n",
    "        ax_left.plot(test_iter_idx, test_loss, '--gp', label = 'Avg_V_Loss')\n",
    "        ax_right = ax_left.twinx()\n",
    "        ax_right.plot(train_iter_idx, train_accuracy, '-bp', label = 'Avg_T_Accuracy')\n",
    "        ax_right.plot(test_iter_idx, test_accuracy, '-yp', label = 'Avg_V_Accuracy')\n",
    "        # ask matplotlib for the plotted objects and their labels\n",
    "        lines_left, labels_left = ax_left.get_legend_handles_labels()\n",
    "        lines_right, labels_right = ax_right.get_legend_handles_labels()\n",
    "        ax_right.legend(lines_left + lines_right, labels_left + labels_right, loc=0)\n",
    "        ax_left.grid()\n",
    "        ax_left.set_xlabel(\"Training Iterations Done(n)\")\n",
    "        ax_left.set_ylabel(\"Loss\")\n",
    "        ax_right.set_ylabel(\"Accuracy\")\n",
    "        ax_right.set_title(\"HVA@({:.3f},{}), LVL@({:.3f},{})\".format(max(test_accuracy),test_iter_idx[test_accuracy.index(max(test_accuracy))],min(test_loss),test_iter_idx[test_loss.index(min(test_loss))]))\n",
    "        plt.savefig(recordDir+'record.png', bbox_inches='tight')\n",
    "        plt.close('all')\n",
    "        train_accu_whole = []\n",
    "        train_loss_whole = []\n",
    "        test_accu_whole = []\n",
    "        test_loss_whole = []\n",
    "        train_accu_whole = open(recordDir+'train_accu.txt').readlines() \n",
    "        train_loss_whole = open(recordDir+'train_loss.txt').readlines()\n",
    "        test_accu_whole = open(recordDir+'test_accu.txt').readlines()\n",
    "        test_loss_whole = open(recordDir+'test_loss.txt').readlines()\n",
    "        train_iter_idx = []\n",
    "        train_loss = []\n",
    "        train_accuracy = []\n",
    "        test_iter_idx = []\n",
    "        test_loss = []\n",
    "        test_accuracy = []\n",
    "        for i in range(0, len(train_accu_whole)):\n",
    "#             if i % test_interval == 0:\n",
    "            #train_iter_idx\n",
    "            train_iter_idx.append(int(train_accu_whole[i].strip('\\r\\n').split(' ')[0]))\n",
    "            #train_loss\n",
    "            train_loss.append(float(train_loss_whole[i].strip('\\r\\n').split(' ')[1]))\n",
    "            #train_accuracy\n",
    "            train_accuracy.append(float(train_accu_whole[i].strip('\\r\\n').split(' ')[1]))\n",
    "        for i in range(0, len(test_accu_whole)):\n",
    "            #test_iter_idx\n",
    "            test_iter_idx.append(int(test_accu_whole[i].strip('\\r\\n').split(' ')[0]))\n",
    "            #test_loss\n",
    "            test_loss.append(float(test_loss_whole[i].strip('\\r\\n').split(' ')[1]))\n",
    "            #test_accuracy\n",
    "            test_accuracy.append(float(test_accu_whole[i].strip('\\r\\n').split(' ')[1]))\n",
    "        fig = plt.figure()\n",
    "        ax_left = fig.add_subplot(111)\n",
    "        ax_left.plot(train_iter_idx, train_loss, '--rp', label = 'Cur_T_Loss')\n",
    "        ax_left.plot(test_iter_idx, test_loss, '--gp', label = 'Avg_V_Loss')\n",
    "        ax_right = ax_left.twinx()\n",
    "        ax_right.plot(train_iter_idx, train_accuracy, '-bp', label = 'Cur_T_Accuracy')\n",
    "        ax_right.plot(test_iter_idx, test_accuracy, '-yp', label = 'Avg_V_Accuracy')\n",
    "        # ask matplotlib for the plotted objects and their labels\n",
    "        lines_left, labels_left = ax_left.get_legend_handles_labels()\n",
    "        lines_right, labels_right = ax_right.get_legend_handles_labels()\n",
    "        ax_right.legend(lines_left + lines_right, labels_left + labels_right, loc=0)\n",
    "        ax_left.grid()\n",
    "        ax_left.set_xlabel(\"Training Iterations Done(n)\")\n",
    "        ax_left.set_ylabel(\"Loss\")\n",
    "        ax_right.set_ylabel(\"Accuracy\")\n",
    "        ax_right.set_title(\"HVA@({:.3f},{}), LVL@({:.3f},{})\".format(max(test_accuracy),test_iter_idx[test_accuracy.index(max(test_accuracy))],min(test_loss),test_iter_idx[test_loss.index(min(test_loss))]))\n",
    "        plt.savefig(recordDir+'cur_record.png', bbox_inches='tight')\n",
    "        plt.close('all')\n",
    "        conf_l_matrix_whole = []\n",
    "        conf_r_matrix_whole = []\n",
    "        conf_l_matrix_whole = open(recordDir+'conf_l_matrix.txt').readlines()\n",
    "        conf_r_matrix_whole = open(recordDir+'conf_r_matrix.txt').readlines()\n",
    "        conf_matrix = []\n",
    "        for i in range(0,len(conf_l_matrix_whole)):\n",
    "            conf_matrix.append([int(conf_r_matrix_whole[i].split(' ')[1]),int(conf_r_matrix_whole[i].split(' ')[2]), \\\n",
    "                                int(conf_l_matrix_whole[i].split(' ')[3]),int(conf_l_matrix_whole[i].split(' ')[4])])\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111)\n",
    "        im = ax.imshow(np.array(conf_matrix).T, interpolation='nearest', cmap=plt.cm.jet)\n",
    "        fig.colorbar(im, ax=ax)\n",
    "        plt.xlabel('Caffemodel Index(n)')\n",
    "        plt.ylabel('like(like,dislike), dislike(like,dislike)')\n",
    "        plt.title('Confusion Matrix@(dislike,like)')\n",
    "        ax.set_aspect(25)\n",
    "        fig.set_size_inches(20.5, 12.5)\n",
    "        fig.savefig(recordDir+'cMat.png',bbox_inches='tight')\n",
    "        plt.close('all')\n",
    "        dd_dislike_whole = []\n",
    "        dl_dislike_whole = []\n",
    "        d_dislike_whole = []\n",
    "        ld_like_whole = []\n",
    "        ll_like_whole = []\n",
    "        l_like_whole = []\n",
    "        dd_dislike_whole = open(recordDir+'dd_dislike.txt').readlines()\n",
    "        dl_dislike_whole = open(recordDir+'dl_dislike.txt').readlines()\n",
    "        d_dislike_whole = open(recordDir+'d_dislike.txt').readlines()\n",
    "        ld_like_whole = open(recordDir+'ld_like.txt').readlines()\n",
    "        ll_like_whole = open(recordDir+'ll_like.txt').readlines()\n",
    "        l_like_whole = open(recordDir+'l_like.txt').readlines()        \n",
    "        iters_idx = []\n",
    "        dd_dislike = []\n",
    "        dd_like = []\n",
    "        dl_dislike = []\n",
    "        dl_like = []\n",
    "        d_dislike = []\n",
    "        d_like = []\n",
    "        ld_like = []\n",
    "        ld_dislike = []\n",
    "        ll_like = []\n",
    "        ll_dislike = []\n",
    "        l_like = []\n",
    "        l_dislike = []\n",
    "        for i in range(0, len(dd_dislike_whole)):\n",
    "            iters_idx.append(int(dd_dislike_whole[i].strip('\\r\\n').split(' ')[0]))\n",
    "            dd_dislike.append(float(dd_dislike_whole[i].strip('\\r\\n').split(' ')[1]))\n",
    "            dd_like.append(1-dd_dislike[i])\n",
    "            dl_dislike.append(float(dl_dislike_whole[i].strip('\\r\\n').split(' ')[1]))\n",
    "            dl_like.append(1-dl_dislike[i])\n",
    "            d_dislike.append(float(d_dislike_whole[i].strip('\\r\\n').split(' ')[1]))\n",
    "            d_like.append(1-d_dislike[i])\n",
    "            ld_like.append(float(ld_like_whole[i].strip('\\r\\n').split(' ')[1]))\n",
    "            ld_dislike.append(1-ld_like[i])\n",
    "            ll_like.append(float(ll_like_whole[i].strip('\\r\\n').split(' ')[1]))\n",
    "            ll_dislike.append(1-ll_like[i])\n",
    "            l_like.append(float(l_like_whole[i].strip('\\r\\n').split(' ')[1]))\n",
    "            l_dislike.append(1-l_like[i])\n",
    "        #dd\n",
    "        fig = plt.figure()\n",
    "        ax_left = fig.add_subplot(111)\n",
    "        ax_left.plot(iters_idx, dd_dislike, '--rp', label = 'dd_dislike')\n",
    "        ax_left.plot(iters_idx, dd_like, '--gp', label = 'dd_like')\n",
    "        lines_left, labels_left = ax_left.get_legend_handles_labels()   \n",
    "        ax_left.legend(lines_left, labels_left, loc=0)\n",
    "        ax_left.grid()\n",
    "        ax_left.set_xlabel(\"Training Iterations Done(n)\")\n",
    "        ax_left.set_ylabel(\"Softmax Probability\")\n",
    "        ax_left.set_title(\"Dislike to Dislike [correct]\")\n",
    "        plt.savefig(recordDir+'dd_record.png', bbox_inches='tight')\n",
    "        plt.close('all')\n",
    "        #dl\n",
    "        fig = plt.figure()\n",
    "        ax_left = fig.add_subplot(111)\n",
    "        ax_left.plot(iters_idx, dl_dislike, '--rp', label = 'dl_dislike')\n",
    "        ax_left.plot(iters_idx, dl_like, '--gp', label = 'dl_like')\n",
    "        lines_left, labels_left = ax_left.get_legend_handles_labels()   \n",
    "        ax_left.legend(lines_left, labels_left, loc=0)\n",
    "        ax_left.grid()\n",
    "        ax_left.set_xlabel(\"Training Iterations Done(n)\")\n",
    "        ax_left.set_ylabel(\"Softmax Probability\")\n",
    "        ax_left.set_title(\"Dislike to Like [wrong]\")\n",
    "        plt.savefig(recordDir+'dl_record.png', bbox_inches='tight')\n",
    "        plt.close('all')\n",
    "        #d\n",
    "        fig = plt.figure()\n",
    "        ax_left = fig.add_subplot(111)\n",
    "        ax_left.plot(iters_idx, d_dislike, '--rp', label = 'd_dislike')\n",
    "        ax_left.plot(iters_idx, d_like, '--gp', label = 'd_like')\n",
    "        lines_left, labels_left = ax_left.get_legend_handles_labels()   \n",
    "        ax_left.legend(lines_left, labels_left, loc=0)\n",
    "        ax_left.grid()\n",
    "        ax_left.set_xlabel(\"Training Iterations Done(n)\")\n",
    "        ax_left.set_ylabel(\"Softmax Probability\")\n",
    "        ax_left.set_title(\"Dislike to Both\")\n",
    "        plt.savefig(recordDir+'d_record.png', bbox_inches='tight')\n",
    "        plt.close('all')\n",
    "        #ld\n",
    "        fig = plt.figure()\n",
    "        ax_left = fig.add_subplot(111)\n",
    "        ax_left.plot(iters_idx, ld_dislike, '--rp', label = 'ld_dislike')\n",
    "        ax_left.plot(iters_idx, ld_like, '--gp', label = 'ld_like')\n",
    "        lines_left, labels_left = ax_left.get_legend_handles_labels()   \n",
    "        ax_left.legend(lines_left, labels_left, loc=0)\n",
    "        ax_left.grid()\n",
    "        ax_left.set_xlabel(\"Training Iterations Done(n)\")\n",
    "        ax_left.set_ylabel(\"Softmax Probability\")\n",
    "        ax_left.set_title(\"Like to Dislike [wrong]\")\n",
    "        plt.savefig(recordDir+'ld_record.png', bbox_inches='tight')\n",
    "        plt.close('all')\n",
    "        #ll\n",
    "        fig = plt.figure()\n",
    "        ax_left = fig.add_subplot(111)\n",
    "        ax_left.plot(iters_idx, ll_dislike, '--rp', label = 'll_dislike')\n",
    "        ax_left.plot(iters_idx, ll_like, '--gp', label = 'll_like')\n",
    "        lines_left, labels_left = ax_left.get_legend_handles_labels()   \n",
    "        ax_left.legend(lines_left, labels_left, loc=0)\n",
    "        ax_left.grid()\n",
    "        ax_left.set_xlabel(\"Training Iterations Done(n)\")\n",
    "        ax_left.set_ylabel(\"Softmax Probability\")\n",
    "        ax_left.set_title(\"Like to Like [correct]\")\n",
    "        plt.savefig(recordDir+'ll_record.png', bbox_inches='tight')\n",
    "        plt.close('all')\n",
    "        #l\n",
    "        fig = plt.figure()\n",
    "        ax_left = fig.add_subplot(111)\n",
    "        ax_left.plot(iters_idx, l_dislike, '--rp', label = 'l_dislike')\n",
    "        ax_left.plot(iters_idx, l_like, '--gp', label = 'l_like')\n",
    "        lines_left, labels_left = ax_left.get_legend_handles_labels()   \n",
    "        ax_left.legend(lines_left, labels_left, loc=0)\n",
    "        ax_left.grid()\n",
    "        ax_left.set_xlabel(\"Training Iterations Done(n)\")\n",
    "        ax_left.set_ylabel(\"Softmax Probability\")\n",
    "        ax_left.set_title(\"Like to Both\")\n",
    "        plt.savefig(recordDir+'l_record.png', bbox_inches='tight')\n",
    "        plt.close('all')\n",
    "        # mean_ndcg & ndcg_at\n",
    "        ndcg_whole = []\n",
    "        ndcg_whole = open(recordDir+'ndcg.txt').readlines()\n",
    "        ndcg_idx = []\n",
    "        mean_ndcg = []\n",
    "        ndcg_1 = []\n",
    "        ndcg_2 = []\n",
    "        ndcg_3 = []\n",
    "        ndcg_4 = []\n",
    "        ndcg_5 = []\n",
    "        ndcg_6 = []\n",
    "        ndcg_7 = []\n",
    "        ndcg_8 = []\n",
    "        ndcg_9 = []\n",
    "        ndcg_10 = []\n",
    "        for i in range(0,len(ndcg_whole)):\n",
    "            if (i % 11 == 0):\n",
    "                ndcg_idx.append(int(ndcg_whole[i+0].strip('\\r\\n').split(' ')[0]))\n",
    "                mean_ndcg.append(float(ndcg_whole[i+0].strip('\\r\\n').split(' ')[1]))\n",
    "                ndcg_1.append(float(ndcg_whole[i+1].strip('\\r\\n').split(' ')[1]))\n",
    "                ndcg_2.append(float(ndcg_whole[i+2].strip('\\r\\n').split(' ')[1]))\n",
    "                ndcg_3.append(float(ndcg_whole[i+3].strip('\\r\\n').split(' ')[1]))\n",
    "                ndcg_4.append(float(ndcg_whole[i+4].strip('\\r\\n').split(' ')[1]))\n",
    "                ndcg_5.append(float(ndcg_whole[i+5].strip('\\r\\n').split(' ')[1]))\n",
    "                ndcg_6.append(float(ndcg_whole[i+6].strip('\\r\\n').split(' ')[1]))\n",
    "                ndcg_7.append(float(ndcg_whole[i+7].strip('\\r\\n').split(' ')[1]))\n",
    "                ndcg_8.append(float(ndcg_whole[i+8].strip('\\r\\n').split(' ')[1]))\n",
    "                ndcg_9.append(float(ndcg_whole[i+9].strip('\\r\\n').split(' ')[1]))\n",
    "                ndcg_10.append(float(ndcg_whole[i+10].strip('\\r\\n').split(' ')[1]))\n",
    "        fig = plt.figure()\n",
    "        ax_left = fig.add_subplot(111)\n",
    "        ax_left.plot(ndcg_idx, mean_ndcg, '--gp', label = 'mean_ndcg')\n",
    "        ax_left.plot(ndcg_idx, ndcg_1, '--r', label = 'ndcg_1')\n",
    "        ax_left.plot(ndcg_idx, ndcg_2, '--g', label = 'ndcg_2')\n",
    "        ax_left.plot(ndcg_idx, ndcg_3, '--b', label = 'ndcg_3')\n",
    "        ax_left.plot(ndcg_idx, ndcg_4, '--c', label = 'ndcg_4')\n",
    "        ax_left.plot(ndcg_idx, ndcg_5, '--m', label = 'ndcg_5')\n",
    "        ax_left.plot(ndcg_idx, ndcg_6, '--k', label = 'ndcg_6')\n",
    "        ax_left.plot(ndcg_idx, ndcg_7, '-.y', label = 'ndcg_7')\n",
    "        ax_left.plot(ndcg_idx, ndcg_8, '-.k', label = 'ndcg_8')\n",
    "        ax_left.plot(ndcg_idx, ndcg_9, '-.b', label = 'ndcg_9')\n",
    "        ax_left.plot(ndcg_idx, ndcg_10, '-.m', label = 'ndcg_10')\n",
    "        lines_left, labels_left = ax_left.get_legend_handles_labels()   \n",
    "        ax_left.legend(lines_left, labels_left, loc=0)\n",
    "        ax_left.grid()\n",
    "        ax_left.set_xlabel(\"Training Iterations Done(n)\")\n",
    "        ax_left.set_ylabel(\"Mean NDCGs\")\n",
    "        ax_left.set_title(\"Mean NDCG of Different Lengths\")\n",
    "        plt.savefig(recordDir+'ndcg.png', bbox_inches='tight')\n",
    "        plt.close('all')\n",
    "        # stop criteria\n",
    "        if test_accu > 0.93:\n",
    "            k = k + 1\n",
    "            if (k > 10):\n",
    "                print '\\n\\nValidation accuracy: {} > 0.93 counted for 10 times\\n\\n'.format(test_accu)\n",
    "                break\n",
    "    # update parameters\n",
    "    solver.step(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#visualization for discovering overfitting\n",
    "\n",
    "#import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.use('Agg') # Must be before importing matplotlib.pyplot or pylab!\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.insert(0,'/home/tonghe2/caffe-master/python')\n",
    "import caffe\n",
    "\n",
    "import os\n",
    "from matplotlib import rc\n",
    "rc('mathtext', default='regular')\n",
    "\n",
    "recordDir = '/home/tonghe2/fashionRecommendation/models/fashionNet_3/training_record/'\n",
    "\n",
    "#os.system('scp '+recordDir + 'train_accu.txt '+'/home/tonghe2/fashionRecommendation/models/fashionNet_3/training_record')\n",
    "#os.system('scp '+recordDir + 'train_loss.txt '+'/home/tonghe2/fashionRecommendation/models/fashionNet_3/training_record')\n",
    "#os.system('scp '+recordDir + 'test_accu.txt '+'/home/tonghe2/fashionRecommendation/models/fashionNet_3/training_record')\n",
    "#os.system('scp '+recordDir + 'test_loss.txt '+'/home/tonghe2/fashionRecommendation/models/fashionNet_3/training_record')\n",
    "\n",
    "train_accu_whole = []\n",
    "train_loss_whole = []\n",
    "test_accu_whole = []\n",
    "test_loss_whole = []\n",
    "\n",
    "train_accu_whole = open(recordDir+'train_accu.txt').readlines() \n",
    "train_loss_whole = open(recordDir+'train_loss.txt').readlines()\n",
    "test_accu_whole = open(recordDir+'test_accu.txt').readlines()\n",
    "test_loss_whole = open(recordDir+'test_loss.txt').readlines()\n",
    "\n",
    "train_iter_idx = []\n",
    "train_loss = []\n",
    "train_accuracy = []\n",
    "\n",
    "test_iter_idx = []\n",
    "test_loss = []\n",
    "test_accuracy = []\n",
    "\n",
    "downSample = 408\n",
    "\n",
    "for i in range(0, len(train_accu_whole)):\n",
    "    if i % downSample == 0: \n",
    "        #train_iter_idx\n",
    "        train_iter_idx.append(int(train_accu_whole[i].strip('\\r\\n').split(' ')[0]))\n",
    "        #train_loss\n",
    "        train_loss.append(float(train_loss_whole[i].strip('\\r\\n').split(' ')[1]))\n",
    "        #train_accuracy\n",
    "        train_accuracy.append(float(train_accu_whole[i].strip('\\r\\n').split(' ')[1]))\n",
    "    \n",
    "for i in range(0, len(test_accu_whole)):\n",
    "    #test_iter_idx\n",
    "    test_iter_idx.append(int(test_accu_whole[i].strip('\\r\\n').split(' ')[0]))\n",
    "    #test_loss\n",
    "    test_loss.append(float(test_loss_whole[i].strip('\\r\\n').split(' ')[1]))\n",
    "    #test_accuracy\n",
    "    test_accuracy.append(float(test_accu_whole[i].strip('\\r\\n').split(' ')[1]))\n",
    "\n",
    "# for i in range(0, 20):\n",
    "#     train_iter_idx.append(i)\n",
    "#     train_loss.append(-2*i+1)\n",
    "#     train_accuracy.append(1.5*i*i)\n",
    "\n",
    "# for i in range(0, 10):\n",
    "#     test_iter_idx.append(2*i)\n",
    "#     test_loss.append(-1.5*i+3)\n",
    "#     test_accuracy.append(0.7*i*i)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax_left = fig.add_subplot(111)\n",
    "ax_left.plot(train_iter_idx, train_loss, '--rp', label = 'Training Loss')\n",
    "ax_left.plot(test_iter_idx, test_loss, '--gp', label = 'Validation Loss')\n",
    "ax_right = ax_left.twinx()\n",
    "ax_right.plot(train_iter_idx, train_accuracy, '-bp', label = 'Training Accuracy')\n",
    "ax_right.plot(test_iter_idx, test_accuracy, '-yp', label = 'Validation Accuracy')\n",
    "\n",
    "# ask matplotlib for the plotted objects and their labels\n",
    "lines_left, labels_left = ax_left.get_legend_handles_labels()\n",
    "lines_right, labels_right = ax_right.get_legend_handles_labels()\n",
    "ax_right.legend(lines_left + lines_right, labels_left + labels_right, loc=0)\n",
    "ax_left.grid()\n",
    "ax_left.set_xlabel(\"Training Iterations (n)\")\n",
    "ax_left.set_ylabel(\"Loss\")\n",
    "ax_right.set_ylabel(\"Accuracy\")\n",
    "ax_right.set_title(\"Hightest Validation Accuracy: {:.3f}, at Training Iteration: {}\".format(max(test_accuracy), test_iter_idx[test_accuracy.index(max(test_accuracy))]))\n",
    "plt.savefig(recordDir+'record_less.png', bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scp tong@ares.cs.ucla.edu:/local2/home/tong/fashionRecommendation/models/fashionNet_3/training_record/record.png /home/tonghe2/fashionRecommendation/models/fashionNet_3/training_record\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#copy parameters solver.net -> net\n",
    "params = net.params.keys()\n",
    "source_params = {pr: (solver.net.params[pr][0].data,solver.net.params[pr][1].data) for pr in params}\n",
    "target_params = {pr: (net.params[pr][0].data,net.params[pr][1].data) for pr in params}\n",
    "for pr in params:\n",
    "    target_params[pr][0][...] = source_params[pr][0] #weights\n",
    "    target_params[pr][1][...] = source_params[pr][1] #bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#verify params copy\n",
    "print 'solver.net: ', solver.net.params['metric_fc1'][0].data\n",
    "print 'net:', solver.net.params['metric_fc1'][0].data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save caffemodel\n",
    "recordDir = '/local2/home/tong/fashionRecommendation/models/fashionNet_3/training_record/'\n",
    "net.save(recordDir+'fashion_params_3_'+str(i+1)+'.caffemodel')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#verify caffemodel\n",
    "net_Verify = caffe.Net('/local2/home/tong/fashionRecommendation/models/fashionNet_3/fashion_deploy_3.prototxt',\n",
    "                recordDir+'fashion_params_3_'+str(i+1)+'.caffemodel',\n",
    "                caffe.TEST)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#verify input data (posi outfit)\n",
    "\n",
    "#input VGG_mean.jpg\n",
    "blob = caffe.proto.caffe_pb2.BlobProto()\n",
    "data = open('./data/VGG_mean.binaryproto', 'rb' ).read()\n",
    "blob.ParseFromString(data)\n",
    "VGG_mean = np.array( caffe.io.blobproto_to_array(blob) )\n",
    "VGG_mean.astype(np.uint8)\n",
    "\n",
    "#get data from blobs\n",
    "top = np.array(solver.net.blobs['top'].data[0])\n",
    "bottom = np.array(solver.net.blobs['bottom'].data[0])\n",
    "shoes = np.array(solver.net.blobs['shoes'].data[0])\n",
    "#change data type to np.uint8\n",
    "top.astype(np.uint8)\n",
    "bottom.astype(np.uint8)\n",
    "shoes.astype(np.uint8)\n",
    "#add VGG_mean\n",
    "top = top + VGG_mean[0,]\n",
    "bottom = bottom + VGG_mean[0,]\n",
    "shoes = shoes + VGG_mean[0,]\n",
    "#BGR to RGB\n",
    "top = np.array([top[2,],top[1,],top[0,]])\n",
    "bottom = np.array([bottom[2,],bottom[1,],bottom[0,]])\n",
    "shoes = np.array([shoes[2,],shoes[1,],shoes[0,]])\n",
    "#(C,H,W) to (H,W,C)\n",
    "top = np.rollaxis(top,0,3)\n",
    "bottom = np.rollaxis(bottom,0,3)\n",
    "shoes = np.rollaxis(shoes,0,3)\n",
    "#temp buffer for using Image.fromarray(*)\n",
    "temp_top = np.zeros(top.shape,np.uint8)\n",
    "temp_bottom = np.zeros(bottom.shape,np.uint8)\n",
    "temp_shoes = np.zeros(shoes.shape,np.uint8)\n",
    "#copy data to buffer\n",
    "for i in range(0,top.shape[0]):\n",
    "    for j in range(0,top.shape[1]):\n",
    "        for k in range(0,top.shape[2]):\n",
    "            temp_top[i,j,k] = top[i,j,k]\n",
    "            temp_bottom[i,j,k] = bottom[i,j,k]\n",
    "            temp_shoes[i,j,k] = shoes[i,j,k]\n",
    "#np.array 2 image\n",
    "import Image\n",
    "img_top = Image.fromarray(temp_top)\n",
    "img_bottom = Image.fromarray(temp_bottom)\n",
    "img_shoes = Image.fromarray(temp_shoes)\n",
    "#save images\n",
    "img_top.save('./top.jpg')\n",
    "img_bottom.save('./bottom.jpg')\n",
    "img_shoes.save('./shoes.jpg')\n",
    "\n",
    "#verify input data (nega outfit)\n",
    "#get data from blobs\n",
    "top = np.array(solver.test_nets[0].blobs['top_n'].data[49])\n",
    "bottom = np.array(solver.test_nets[0].blobs['bottom_n'].data[49])\n",
    "shoes = np.array(solver.test_nets[0].blobs['shoes_n'].data[49])\n",
    "#change data type to np.uint8\n",
    "top.astype(np.uint8)\n",
    "bottom.astype(np.uint8)\n",
    "shoes.astype(np.uint8)\n",
    "#add VGG_mean\n",
    "top = top + VGG_mean\n",
    "bottom = bottom + VGG_mean\n",
    "shoes = shoes + VGG_mean\n",
    "#BGR to RGB\n",
    "top = np.array([top[2,],top[1,],top[0,]])\n",
    "bottom = np.array([bottom[2,],bottom[1,],bottom[0,]])\n",
    "shoes = np.array([shoes[2,],shoes[1,],shoes[0,]])\n",
    "#(C,H,W) to (H,W,C)\n",
    "top = np.rollaxis(top,0,3)\n",
    "bottom = np.rollaxis(bottom,0,3)\n",
    "shoes = np.rollaxis(shoes,0,3)\n",
    "#temp buffer for using Image.fromarray(*)\n",
    "temp_top = np.zeros(top.shape,np.uint8)\n",
    "temp_bottom = np.zeros(bottom.shape,np.uint8)\n",
    "temp_shoes = np.zeros(shoes.shape,np.uint8)\n",
    "#copy data to buffer\n",
    "for i in range(0,top.shape[0]):\n",
    "    for j in range(0,top.shape[1]):\n",
    "        for k in range(0,top.shape[2]):\n",
    "            temp_top[i,j,k] = top[i,j,k]\n",
    "            temp_bottom[i,j,k] = bottom[i,j,k]\n",
    "            temp_shoes[i,j,k] = shoes[i,j,k]\n",
    "#np.array 2 image\n",
    "import Image\n",
    "img_top = Image.fromarray(temp_top)\n",
    "img_bottom = Image.fromarray(temp_bottom)\n",
    "img_shoes = Image.fromarray(temp_shoes)\n",
    "#save images\n",
    "img_top.save('./top_n.jpg')\n",
    "img_bottom.save('./bottom_n.jpg')\n",
    "img_shoes.save('./shoes_n.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
