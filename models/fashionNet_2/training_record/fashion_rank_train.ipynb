{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.use('Agg') # Must be before importing matplotlib.pyplot or pylab!\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.insert(0,'/local2/home/tong/caffe-master/python')\n",
    "import caffe\n",
    "\n",
    "import os\n",
    "from matplotlib import rc\n",
    "rc('mathtext', default='regular')\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "caffe.set_mode_gpu()\n",
    "caffe.set_device(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for training\n",
    "solver = caffe.SGDSolver('/local2/home/tong/fashionRecommendation/models/fashionNet_3/fashion_solver_3.prototxt')\n",
    "#CHANGE!!!\n",
    "# solver.net.copy_from('/local2/home/tong/fashionRecommendation/models/VGG_M_2048/VGG_CNN_M_2048.caffemodel')\n",
    "#CHANGE!!!\n",
    "# solver.test_nets[0].copy_from('/local2/home/tong/fashionRecommendation/models/VGG_M_2048/VGG_CNN_M_2048.caffemodel')\n",
    "# solver.net.copy_from('/local2/home/tong/fashionRecommendation/models/fashionNet_2/training_record/fashion_params_2_91448.caffemodel')\n",
    "# solver.test_nets[0].copy_from('/local2/home/tong/fashionRecommendation/models/fashionNet_2/training_record/fashion_params_2_91448.caffemodel')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_avg(test_iter,iters):\n",
    "    avg_accu = 0\n",
    "    avg_loss = 0\n",
    "    y_l_true = []\n",
    "    y_l_pred = []\n",
    "    y_r_true = []\n",
    "    y_r_pred = []\n",
    "    dd_dislike = 0\n",
    "    dd_count = 0\n",
    "    dl_dislike = 0\n",
    "    dl_count = 0\n",
    "    ld_like = 0\n",
    "    ld_count = 0\n",
    "    ll_like = 0\n",
    "    ll_count = 0\n",
    "    nr_tuples_pos = []\n",
    "    count_posi = 0\n",
    "    nr_tuples_neg = []\n",
    "    count_nega = 0\n",
    "    scores_pos = []\n",
    "    scores_neg = []\n",
    "    for i in range(0,test_iter):\n",
    "        if (i%50==0 or (i==(test_iter-1))):\n",
    "            print(\"{}/{} @ {}\\n\".format(i+1,test_iter,iters))\n",
    "        solver.test_nets[0].forward()\n",
    "        avg_accu = avg_accu + solver.test_nets[0].blobs['accuracy'].data\n",
    "        avg_loss = avg_loss + solver.test_nets[0].blobs['rank_Loss'].data\n",
    "        for j in range(0,len(solver.test_nets[0].blobs['label_top'].data)):\n",
    "            y_l_pred.append(0 if solver.test_nets[0].blobs['metric_fc3'].data[j][0]>solver.test_nets[0].blobs['metric_fc3'].data[j][1] else 1)\n",
    "            y_l_true.append(1)\n",
    "            # count posi tuple number\n",
    "            count_posi = count_posi + 1\n",
    "            # record pre_score for this posi outfit\n",
    "            scores_pos.append(solver.test_nets[0].blobs['metric_fc3_softmax'].data[j][1])\n",
    "            y_r_pred.append(0 if solver.test_nets[0].blobs['metric_fc3_n'].data[j][0]>solver.test_nets[0].blobs['metric_fc3_n'].data[j][1] else 1)\n",
    "            y_r_true.append(0)\n",
    "            # count neg tuple number\n",
    "            count_nega = count_nega + 1\n",
    "            # record pre_score for this nega outfit\n",
    "            scores_neg.append(solver.test_nets[0].blobs['metric_fc3_softmax_n'].data[j][1])\n",
    "            #dd\n",
    "            if (solver.test_nets[0].blobs['metric_fc3_n'].data[j][0]>solver.test_nets[0].blobs['metric_fc3_n'].data[j][1]):\n",
    "                dd_count = dd_count + 1\n",
    "                dd_dislike = dd_dislike + solver.test_nets[0].blobs['metric_fc3_softmax_n'].data[j][0]\n",
    "            #dl\n",
    "            else:\n",
    "                dl_count = dl_count + 1\n",
    "                dl_dislike = dl_dislike + solver.test_nets[0].blobs['metric_fc3_softmax_n'].data[j][0]\n",
    "            #ld\n",
    "            if (solver.test_nets[0].blobs['metric_fc3'].data[j][0]>solver.test_nets[0].blobs['metric_fc3'].data[j][1]):\n",
    "                ld_count = ld_count + 1\n",
    "                ld_like = ld_like + solver.test_nets[0].blobs['metric_fc3_softmax'].data[j][1]\n",
    "            #ll\n",
    "            else:\n",
    "                ll_count = ll_count + 1\n",
    "                ll_like = ll_like + solver.test_nets[0].blobs['metric_fc3_softmax'].data[j][1]\n",
    "    cMat_l = confusion_matrix(y_l_true, y_l_pred)\n",
    "    if(len(cMat_l)==1):\n",
    "        cMat_l = [[0,0],[0,cMat_l[0][0]]]\n",
    "    cMat_r = confusion_matrix(y_r_true, y_r_pred)\n",
    "    if(len(cMat_r)==1):\n",
    "        cMat_r = [[cMat_r[0][0],0],[0,0]]\n",
    "    nr_tuples_pos.append(count_posi)\n",
    "    nr_tuples_neg.append(count_nega)\n",
    "    \n",
    "    return (avg_accu/test_iter), (avg_loss/test_iter),cMat_l,cMat_r, \\\n",
    "           (0 if dd_count==0 else dd_dislike/dd_count), (0 if dl_count==0 else dl_dislike/dl_count), ((dd_dislike+dl_dislike)/(dd_count+dl_count)), \\\n",
    "           (0 if ld_count==0 else ld_like/ld_count), (0 if ll_count==0 else ll_like/ll_count), ((ld_like+ll_like)/(ld_count+ll_count)), \\\n",
    "           scores_pos, scores_neg, nr_tuples_pos, nr_tuples_neg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_ndcg(scores_pos, scores_neg, nr_tuples_pos, nr_tuples_neg,\n",
    "\t\t\t fn_out='', tuples_pos=None, tuples_neg=None, nr_return=0):\n",
    "\t\"\"\"\n",
    "\t- nr_tuples_pos[ui]: the number of positive outfits for user ui\n",
    "\t- nr_tuples_neg[ui]: the number of negative(neutral) outfits for user ui\n",
    "\t\n",
    "\t- scores_pos: scores for positive outfits. \n",
    "\t\t\t\t  scores_pos[0:nr_tuples_pos[0]] are the scores for the first user.\n",
    "\t\t\t\t  scores_pos[nr_tuples_pos[0]:nr_tuples_pos[0]+nr_tuples_pos[1]] are the scores for the second user.\n",
    "\t\t\t\t\n",
    "\t- scores_neg: scores for negative outfits. \n",
    "\t\t\t\t  scores_neg[0:nr_tuples_neg[0]] are the scores for the first user.\n",
    "\t\t\t\t  scores_neg[nr_tuples_neg[0]:nr_tuples_neg[0]+nr_tuples_neg[1]] are the scores for the second user.\n",
    "\t\"\"\"\n",
    "\tif fn_out != '':\n",
    "            fid_out = open(fn_out, 'w')\n",
    "\n",
    "\tm = 10 # pre-determined ndcg size\n",
    "\tndcg_ct = np.zeros(m)\n",
    "\tndcg_at = np.zeros(m)\n",
    "\tmean_ndcg = 0\n",
    "\ts_ind_pos = 0 # update posi outfit index for each user\n",
    "\ts_ind_neg = 0 # update neutral outfit index for each user\n",
    "\tnr_users = len(nr_tuples_pos) # user number\n",
    "\tfor ui in range(nr_users): # for each user\n",
    "\t\tcount_q = nr_tuples_pos[ui] + nr_tuples_neg[ui] # total outfits (both posi & neutral) number of this user\n",
    "\t\tlabel = np.zeros(nr_tuples_pos[ui]+nr_tuples_neg[ui]) # labels for all outfits of this user, 1 for posi & 0 for neutral\n",
    "\t\tlabel[:nr_tuples_pos[ui]] = 1\n",
    "\t\ttarget = np.empty(nr_tuples_pos[ui]+nr_tuples_neg[ui]) # scores for all outfits (both posi & neutral)\n",
    "\t\ttarget[:nr_tuples_pos[ui]] = scores_pos[s_ind_pos:s_ind_pos+nr_tuples_pos[ui]] # scores for posi outfits\n",
    "\t\ttarget[nr_tuples_pos[ui]:] = scores_neg[s_ind_neg:s_ind_neg+nr_tuples_neg[ui]] # scores for neutral outfits\n",
    "\t\tif fn_out != '':\n",
    "\t\t\ttuples = np.hstack((tuples_pos[:,s_ind_pos:s_ind_pos+nr_tuples_pos[ui]],\n",
    "\t\t\t\t\t\t\t\ttuples_neg[:,s_ind_neg:s_ind_neg+nr_tuples_neg[ui]]))\n",
    "\t\ts_ind_pos += nr_tuples_pos[ui]\n",
    "\t\ts_ind_neg += nr_tuples_neg[ui]\n",
    "\n",
    "\t\tndcg_size = min(m, count_q) # actual ndcg size\n",
    "\t\tideal_dcg = np.empty(count_q) # for computing ideal ndcg value\n",
    "\t\tdcg = np.empty(count_q) # for computing dcg (without normalization by N_m yet)\n",
    "\t\tndcg = 0\n",
    "\t\torder = np.argsort(-label) # sort label in descending order, returns the sequential indices of label \n",
    "\t\tideal_dcg[0] = pow(2.0, label[order[0]]) - 1 # compute ideal_ndcg@m (m=1)\n",
    "\t\tfor i in range(1, count_q): # compute ideal_ndcg@m (m=2,...,M), M is total outfits number of this user\n",
    "\t\t\tideal_dcg[i] = ideal_dcg[i-1]+(pow(2.0, label[order[i]])\n",
    "\t\t\t\t\t\t\t\t\t\t   - 1)*np.log(2.0)/np.log(i+1.0)\n",
    "\t\torder = np.argsort(-target) # sort scores for all outfits in descending order, returns the indices\n",
    "\t\tdcg[0] = pow(2.0, label[order[0]]) - 1 # compute dcg@m (m=1)\n",
    "\t\tfor i in range(1, count_q): # compute dcg@m (m=2,...,M), M is total outfits number of this user\n",
    "\t\t\tdcg[i] = dcg[i-1]+(pow(2.0, label[order[i]])\n",
    "\t\t\t\t\t\t\t   - 1)*np.log(2.0)/np.log(i+1.0)\n",
    "\t\tif ideal_dcg[0] > 0: # at least there should be one posi outfit for this user, or else somehting is wrong here\n",
    "\t\t\tfor i in range(count_q): # for each @m (m=1,...,M)\n",
    "\t\t\t\tndcg += dcg[i] / ideal_dcg[i] # add up all ndcg@m (m=1,..,M) for this user\n",
    "\t\t\tfor i in range(ndcg_size): # compute top 10 ndcg for all users\n",
    "\t\t\t\tndcg_ct[i] += 1 # record outfits num at each place, among top 10\n",
    "\t\t\t\tndcg_at[i] += dcg[i] / ideal_dcg[i] # add up ndcg value at each place, among top 10\n",
    "\t\telse: # if we only have no posi outfit for this user\n",
    "\t\t\tndcg = 0 # ndcg is 0 for this user, because there is not point of ranking anymore for him/here\n",
    "\t\tm_ndcg = ndcg / count_q # mean ndcg for this user\n",
    "\t\tmean_ndcg += m_ndcg # add up mean ndcg for all users\n",
    "\n",
    "\t\tif fn_out != '':\n",
    "\t\t\tfid_out.write('%f\\n' % m_ndcg)\n",
    "\t\t\tn_out = min(count_q, nr_return)\n",
    "\t\t\tfor i in range(n_out):\n",
    "\t\t\t\tfid_out.write('%d ' % label[order[i]])\n",
    "\t\t\t\tfor jj in range(tuples.shape[0]):\n",
    "\t\t\t\t\tfid_out.write('%d ' % tuples[jj, order[i]])\n",
    "\t\t\t\tfid_out.write('\\n')\n",
    "\n",
    "\tmean_ndcg /= nr_users # mean ndcg for all users as a whole\n",
    "\tfor i in range(m): # top 10 mean ndcg for all users as a whole\n",
    "\t\tndcg_at[i] /= ndcg_ct[i]\n",
    "\n",
    "\tif fn_out != '':\n",
    "\t\tfid_out.close()\n",
    "\n",
    "\treturn (mean_ndcg, ndcg_at)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test_iter = 450\n",
    "# test_accu,test_loss,cMat_l,cMat_r=test_avg(test_iter)\n",
    "# print(\"test_accu: {}\\n\".format(test_accu))\n",
    "# print(\"test_loss: {}\\n\".format(test_loss))\n",
    "# print(\"cMat_l:\\n\")\n",
    "# print(cMat_l)\n",
    "# print(\"\\ncMat_r:\\n\")\n",
    "# print(cMat_r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "recordDir = '/local2/home/tong/fashionRecommendation/models/fashionNet_2/training_record/'\n",
    "test_interval = 920 # 1/10 train epoch\n",
    "test_iter = 90 # 1/5 test epoch\n",
    "start_iter = 0\n",
    "end_iter = 137706 #7+8 epoch\n",
    "for i in range(start_iter,end_iter+1):\n",
    "    if (i%test_interval==0 or i==end_iter):\n",
    "        #validation & conf_matrix\n",
    "        solver.test_nets[0].copy_from(recordDir+'fashion_params_2_'+str(i)+'.caffemodel')\n",
    "        test_accu,test_loss,cMat_l,cMat_r,dd_dislike,dl_dislike,d_dislike,ld_like,ll_like,l_like,scores_pos, scores_neg, nr_tuples_pos, nr_tuples_neg=test_avg(test_iter,i)\n",
    "        mean_ndcg, ndcg_at = get_ndcg(scores_pos, scores_neg, nr_tuples_pos, nr_tuples_neg)\n",
    "        test_rank_accu = open(recordDir + 'test_rank_accu.txt','a')\n",
    "        test_rank_loss = open(recordDir + 'test_rank_loss.txt','a')\n",
    "        test_rank_accu.write(str(i)+' '+str(test_accu)+'\\r\\n')\n",
    "        test_rank_loss.write(str(i)+' '+str(test_loss)+'\\r\\n')\n",
    "        test_rank_accu.close()\n",
    "        test_rank_loss.close()\n",
    "        te_l_conf = open(recordDir + 'conf_l_matrix.txt','a')\n",
    "        te_l_conf.write(str(i)+' '+str(cMat_l[0][0])+' '+str(cMat_l[0][1])+' '+str(cMat_l[1][0])+' '+str(cMat_l[1][1])+'\\r\\n')\n",
    "        te_l_conf.close()\n",
    "        te_r_conf = open(recordDir + 'conf_r_matrix.txt','a')\n",
    "        te_r_conf.write(str(i)+' '+str(cMat_r[0][0])+' '+str(cMat_r[0][1])+' '+str(cMat_r[1][0])+' '+str(cMat_r[1][1])+'\\r\\n')\n",
    "        te_r_conf.close()\n",
    "        dd_f = open(recordDir + 't2.1_rank/dd_dislike.txt','a')\n",
    "        dd_f.write(str(i)+' '+str(dd_dislike)+'\\r\\n')\n",
    "        dd_f.close()\n",
    "        dl_f = open(recordDir + 't2.1_rank/dl_dislike.txt','a')\n",
    "        dl_f.write(str(i)+' '+str(dl_dislike)+'\\r\\n')\n",
    "        dl_f.close()\n",
    "        d_f = open(recordDir + 't2.1_rank/d_dislike.txt','a')\n",
    "        d_f.write(str(i)+' '+str(d_dislike)+'\\r\\n')\n",
    "        d_f.close()\n",
    "        ld_f = open(recordDir + 't2.1_rank/ld_like.txt','a')\n",
    "        ld_f.write(str(i)+' '+str(ld_like)+'\\r\\n')\n",
    "        ld_f.close()\n",
    "        ll_f = open(recordDir + 't2.1_rank/ll_like.txt','a')\n",
    "        ll_f.write(str(i)+' '+str(ll_like)+'\\r\\n')\n",
    "        ll_f.close()\n",
    "        l_f = open(recordDir + 't2.1_rank/l_like.txt','a')\n",
    "        l_f.write(str(i)+' '+str(l_like)+'\\r\\n')\n",
    "        l_f.close()\n",
    "        # record mean_ndcg & ndcg_at\n",
    "        ndcg_f = open(recordDir+'t2.1_rank/ndcg.txt','a')\n",
    "        ndcg_f.write(str(i)+' '+str(mean_ndcg)+'\\r\\n')\n",
    "        for n in range(0,len(ndcg_at)):\n",
    "            ndcg_f.write(str(i)+' '+str(ndcg_at[n])+'\\r\\n')\n",
    "        ndcg_f.close()\n",
    "        print(\"\\nIters done:{}, VAL_accu={}, VAL_loss={}.\\n\".format(i,test_accu,test_loss))\n",
    "        #visualization\n",
    "        test_accu_whole = []\n",
    "        test_loss_whole = []\n",
    "        test_accu_whole = open(recordDir+'test_rank_accu.txt').readlines()\n",
    "        test_loss_whole = open(recordDir+'test_rank_loss.txt').readlines()\n",
    "        test_iter_idx = []\n",
    "        test_loss = []\n",
    "        test_accuracy = []\n",
    "        for i in range(0, len(test_accu_whole)):\n",
    "            #test_iter_idx\n",
    "            test_iter_idx.append(int(test_accu_whole[i].strip('\\r\\n').split(' ')[0]))\n",
    "            #test_loss\n",
    "            test_loss.append(float(test_loss_whole[i].strip('\\r\\n').split(' ')[1]))\n",
    "            #test_accuracy\n",
    "            test_accuracy.append(float(test_accu_whole[i].strip('\\r\\n').split(' ')[1]))\n",
    "        fig = plt.figure()\n",
    "        ax_left = fig.add_subplot(111)\n",
    "        ax_left.plot(test_iter_idx, test_loss, '--gp', label = 'Avg_V_Loss')\n",
    "        ax_right = ax_left.twinx()\n",
    "        ax_right.plot(test_iter_idx, test_accuracy, '-yp', label = 'Avg_V_Accuracy')\n",
    "        # ask matplotlib for the plotted objects and their labels\n",
    "        lines_left, labels_left = ax_left.get_legend_handles_labels()\n",
    "        lines_right, labels_right = ax_right.get_legend_handles_labels()\n",
    "        ax_right.legend(lines_left + lines_right, labels_left + labels_right, loc=0)\n",
    "        ax_left.grid()\n",
    "        ax_left.set_xlabel(\"Training Iterations Done(n)\")\n",
    "        ax_left.set_ylabel(\"Loss\")\n",
    "        ax_right.set_ylabel(\"Accuracy\")\n",
    "        ax_right.set_title(\"HVA@({:.3f},{}), LVL@({:.3f},{})\".format(max(test_accuracy),test_iter_idx[test_accuracy.index(max(test_accuracy))],min(test_loss),test_iter_idx[test_loss.index(min(test_loss))]))\n",
    "        plt.savefig(recordDir+'rank_record.png', bbox_inches='tight')\n",
    "        plt.close('all')\n",
    "        conf_l_matrix_whole = []\n",
    "        conf_r_matrix_whole = []\n",
    "        conf_l_matrix_whole = open(recordDir+'conf_l_matrix.txt').readlines()\n",
    "        conf_r_matrix_whole = open(recordDir+'conf_r_matrix.txt').readlines()\n",
    "        conf_matrix = []\n",
    "        for i in range(0,len(conf_l_matrix_whole)):\n",
    "            conf_matrix.append([int(conf_r_matrix_whole[i].split(' ')[1]),int(conf_r_matrix_whole[i].split(' ')[2]), \\\n",
    "                                int(conf_l_matrix_whole[i].split(' ')[3]),int(conf_l_matrix_whole[i].split(' ')[4])])\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111)\n",
    "        im = ax.imshow(np.array(conf_matrix).T, interpolation='nearest', cmap=plt.cm.jet)\n",
    "        fig.colorbar(im, ax=ax)\n",
    "        plt.xlabel('Caffemodel Index(n)')\n",
    "        plt.ylabel('like(like,dislike), dislike(like,dislike)')\n",
    "        plt.title('Confusion Matrix@(dislike,like)')\n",
    "        ax.set_aspect(25)\n",
    "        fig.set_size_inches(20.5, 12.5)\n",
    "        fig.savefig(recordDir+'t2.1_rank/cMat.png',bbox_inches='tight')\n",
    "        plt.close('all')\n",
    "        dd_dislike_whole = []\n",
    "        dl_dislike_whole = []\n",
    "        d_dislike_whole = []\n",
    "        ld_like_whole = []\n",
    "        ll_like_whole = []\n",
    "        l_like_whole = []\n",
    "        dd_dislike_whole = open(recordDir+'t2.1_rank/dd_dislike.txt').readlines()\n",
    "        dl_dislike_whole = open(recordDir+'t2.1_rank/dl_dislike.txt').readlines()\n",
    "        d_dislike_whole = open(recordDir+'t2.1_rank/d_dislike.txt').readlines()\n",
    "        ld_like_whole = open(recordDir+'t2.1_rank/ld_like.txt').readlines()\n",
    "        ll_like_whole = open(recordDir+'t2.1_rank/ll_like.txt').readlines()\n",
    "        l_like_whole = open(recordDir+'t2.1_rank/l_like.txt').readlines()        \n",
    "        iters_idx = []\n",
    "        dd_dislike = []\n",
    "        dd_like = []\n",
    "        dl_dislike = []\n",
    "        dl_like = []\n",
    "        d_dislike = []\n",
    "        d_like = []\n",
    "        ld_like = []\n",
    "        ld_dislike = []\n",
    "        ll_like = []\n",
    "        ll_dislike = []\n",
    "        l_like = []\n",
    "        l_dislike = []\n",
    "        for i in range(0, len(dd_dislike_whole)):\n",
    "            iters_idx.append(int(dd_dislike_whole[i].strip('\\r\\n').split(' ')[0]))\n",
    "            dd_dislike.append(float(dd_dislike_whole[i].strip('\\r\\n').split(' ')[1]))\n",
    "            dd_like.append(1-dd_dislike[i])\n",
    "            dl_dislike.append(float(dl_dislike_whole[i].strip('\\r\\n').split(' ')[1]))\n",
    "            dl_like.append(1-dl_dislike[i])\n",
    "            d_dislike.append(float(d_dislike_whole[i].strip('\\r\\n').split(' ')[1]))\n",
    "            d_like.append(1-d_dislike[i])\n",
    "            ld_like.append(float(ld_like_whole[i].strip('\\r\\n').split(' ')[1]))\n",
    "            ld_dislike.append(1-ld_like[i])\n",
    "            ll_like.append(float(ll_like_whole[i].strip('\\r\\n').split(' ')[1]))\n",
    "            ll_dislike.append(1-ll_like[i])\n",
    "            l_like.append(float(l_like_whole[i].strip('\\r\\n').split(' ')[1]))\n",
    "            l_dislike.append(1-l_like[i])\n",
    "        #dd\n",
    "        fig = plt.figure()\n",
    "        ax_left = fig.add_subplot(111)\n",
    "        ax_left.plot(iters_idx, dd_dislike, '--rp', label = 'dd_dislike')\n",
    "        ax_left.plot(iters_idx, dd_like, '--gp', label = 'dd_like')\n",
    "        lines_left, labels_left = ax_left.get_legend_handles_labels()   \n",
    "        ax_left.legend(lines_left, labels_left, loc=0)\n",
    "        ax_left.grid()\n",
    "        ax_left.set_xlabel(\"Training Iterations Done(n)\")\n",
    "        ax_left.set_ylabel(\"Softmax Probability\")\n",
    "        ax_left.set_title(\"Dislike to Dislike [correct]\")\n",
    "        plt.savefig(recordDir+'t2.1_rank/dd_record.png', bbox_inches='tight')\n",
    "        plt.close('all')\n",
    "        #dl\n",
    "        fig = plt.figure()\n",
    "        ax_left = fig.add_subplot(111)\n",
    "        ax_left.plot(iters_idx, dl_dislike, '--rp', label = 'dl_dislike')\n",
    "        ax_left.plot(iters_idx, dl_like, '--gp', label = 'dl_like')\n",
    "        lines_left, labels_left = ax_left.get_legend_handles_labels()   \n",
    "        ax_left.legend(lines_left, labels_left, loc=0)\n",
    "        ax_left.grid()\n",
    "        ax_left.set_xlabel(\"Training Iterations Done(n)\")\n",
    "        ax_left.set_ylabel(\"Softmax Probability\")\n",
    "        ax_left.set_title(\"Dislike to Like [wrong]\")\n",
    "        plt.savefig(recordDir+'t2.1_rank/dl_record.png', bbox_inches='tight')        \n",
    "        plt.close('all')\n",
    "        #d\n",
    "        fig = plt.figure()\n",
    "        ax_left = fig.add_subplot(111)\n",
    "        ax_left.plot(iters_idx, d_dislike, '--rp', label = 'd_dislike')\n",
    "        ax_left.plot(iters_idx, d_like, '--gp', label = 'd_like')\n",
    "        lines_left, labels_left = ax_left.get_legend_handles_labels()   \n",
    "        ax_left.legend(lines_left, labels_left, loc=0)\n",
    "        ax_left.grid()\n",
    "        ax_left.set_xlabel(\"Training Iterations Done(n)\")\n",
    "        ax_left.set_ylabel(\"Softmax Probability\")\n",
    "        ax_left.set_title(\"Dislike to Both\")\n",
    "        plt.savefig(recordDir+'t2.1_rank/d_record.png', bbox_inches='tight')\n",
    "        plt.close('all')\n",
    "        #ld\n",
    "        fig = plt.figure()\n",
    "        ax_left = fig.add_subplot(111)\n",
    "        ax_left.plot(iters_idx, ld_dislike, '--rp', label = 'ld_dislike')\n",
    "        ax_left.plot(iters_idx, ld_like, '--gp', label = 'ld_like')\n",
    "        lines_left, labels_left = ax_left.get_legend_handles_labels()   \n",
    "        ax_left.legend(lines_left, labels_left, loc=0)\n",
    "        ax_left.grid()\n",
    "        ax_left.set_xlabel(\"Training Iterations Done(n)\")\n",
    "        ax_left.set_ylabel(\"Softmax Probability\")\n",
    "        ax_left.set_title(\"Like to Dislike [wrong]\")\n",
    "        plt.savefig(recordDir+'t2.1_rank/ld_record.png', bbox_inches='tight')\n",
    "        plt.close('all')\n",
    "        #ll\n",
    "        fig = plt.figure()\n",
    "        ax_left = fig.add_subplot(111)\n",
    "        ax_left.plot(iters_idx, ll_dislike, '--rp', label = 'll_dislike')\n",
    "        ax_left.plot(iters_idx, ll_like, '--gp', label = 'll_like')\n",
    "        lines_left, labels_left = ax_left.get_legend_handles_labels()   \n",
    "        ax_left.legend(lines_left, labels_left, loc=0)\n",
    "        ax_left.grid()\n",
    "        ax_left.set_xlabel(\"Training Iterations Done(n)\")\n",
    "        ax_left.set_ylabel(\"Softmax Probability\")\n",
    "        ax_left.set_title(\"Like to Like [correct]\")\n",
    "        plt.savefig(recordDir+'t2.1_rank/ll_record.png', bbox_inches='tight')\n",
    "        plt.close('all')\n",
    "        #l\n",
    "        fig = plt.figure()\n",
    "        ax_left = fig.add_subplot(111)\n",
    "        ax_left.plot(iters_idx, l_dislike, '--rp', label = 'l_dislike')\n",
    "        ax_left.plot(iters_idx, l_like, '--gp', label = 'l_like')\n",
    "        lines_left, labels_left = ax_left.get_legend_handles_labels()   \n",
    "        ax_left.legend(lines_left, labels_left, loc=0)\n",
    "        ax_left.grid()\n",
    "        ax_left.set_xlabel(\"Training Iterations Done(n)\")\n",
    "        ax_left.set_ylabel(\"Softmax Probability\")\n",
    "        ax_left.set_title(\"Like to Both\")\n",
    "        plt.savefig(recordDir+'t2.1_rank/l_record.png', bbox_inches='tight')\n",
    "        plt.close('all')\n",
    "        # mean_ndcg & ndcg_at\n",
    "        ndcg_whole = []\n",
    "        ndcg_whole = open(recordDir+'t2.1_rank/ndcg.txt').readlines()\n",
    "        ndcg_idx = []\n",
    "        mean_ndcg = []\n",
    "        ndcg_1 = []\n",
    "        ndcg_2 = []\n",
    "        ndcg_3 = []\n",
    "        ndcg_4 = []\n",
    "        ndcg_5 = []\n",
    "        ndcg_6 = []\n",
    "        ndcg_7 = []\n",
    "        ndcg_8 = []\n",
    "        ndcg_9 = []\n",
    "        ndcg_10 = []\n",
    "        for i in range(0,len(ndcg_whole)):\n",
    "            if (i % 11 == 0):\n",
    "                ndcg_idx.append(int(ndcg_whole[i+0].strip('\\r\\n').split(' ')[0]))\n",
    "                mean_ndcg.append(float(ndcg_whole[i+0].strip('\\r\\n').split(' ')[1]))\n",
    "                ndcg_1.append(float(ndcg_whole[i+1].strip('\\r\\n').split(' ')[1]))\n",
    "                ndcg_2.append(float(ndcg_whole[i+2].strip('\\r\\n').split(' ')[1]))\n",
    "                ndcg_3.append(float(ndcg_whole[i+3].strip('\\r\\n').split(' ')[1]))\n",
    "                ndcg_4.append(float(ndcg_whole[i+4].strip('\\r\\n').split(' ')[1]))\n",
    "                ndcg_5.append(float(ndcg_whole[i+5].strip('\\r\\n').split(' ')[1]))\n",
    "                ndcg_6.append(float(ndcg_whole[i+6].strip('\\r\\n').split(' ')[1]))\n",
    "                ndcg_7.append(float(ndcg_whole[i+7].strip('\\r\\n').split(' ')[1]))\n",
    "                ndcg_8.append(float(ndcg_whole[i+8].strip('\\r\\n').split(' ')[1]))\n",
    "                ndcg_9.append(float(ndcg_whole[i+9].strip('\\r\\n').split(' ')[1]))\n",
    "                ndcg_10.append(float(ndcg_whole[i+10].strip('\\r\\n').split(' ')[1]))\n",
    "        fig = plt.figure()\n",
    "        ax_left = fig.add_subplot(111)\n",
    "        ax_left.plot(ndcg_idx, mean_ndcg, '--gp', label = 'mean_ndcg')\n",
    "        ax_left.plot(ndcg_idx, ndcg_1, '--r', label = 'ndcg_1')\n",
    "        ax_left.plot(ndcg_idx, ndcg_2, '--g', label = 'ndcg_2')\n",
    "        ax_left.plot(ndcg_idx, ndcg_3, '--b', label = 'ndcg_3')\n",
    "        ax_left.plot(ndcg_idx, ndcg_4, '--c', label = 'ndcg_4')\n",
    "        ax_left.plot(ndcg_idx, ndcg_5, '--m', label = 'ndcg_5')\n",
    "        ax_left.plot(ndcg_idx, ndcg_6, '--k', label = 'ndcg_6')\n",
    "        ax_left.plot(ndcg_idx, ndcg_7, '-.y', label = 'ndcg_7')\n",
    "        ax_left.plot(ndcg_idx, ndcg_8, '-.k', label = 'ndcg_8')\n",
    "        ax_left.plot(ndcg_idx, ndcg_9, '-.b', label = 'ndcg_9')\n",
    "        ax_left.plot(ndcg_idx, ndcg_10, '-.m', label = 'ndcg_10')\n",
    "        lines_left, labels_left = ax_left.get_legend_handles_labels()   \n",
    "        ax_left.legend(lines_left, labels_left, loc=0)\n",
    "        ax_left.grid()\n",
    "        ax_left.set_xlabel(\"Training Iterations Done(n)\")\n",
    "        ax_left.set_ylabel(\"Mean NDCGs\")\n",
    "        ax_left.set_title(\"Mean NDCG of Different Lengths\")\n",
    "        plt.savefig(recordDir+'t2.1_rank/ndcg.png', bbox_inches='tight')\n",
    "        plt.close('all')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('Agg') # Must be before importing matplotlib.pyplot or pylab!\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "recordDir = '/local2/home/tong/fashionRecommendation/models/fashionNet_2/training_record/'\n",
    "\n",
    "conf_l_matrix_whole = []\n",
    "conf_r_matrix_whole = []\n",
    "conf_l_matrix_whole = open(recordDir+'conf_l_matrix.txt').readlines()\n",
    "conf_r_matrix_whole = open(recordDir+'conf_r_matrix.txt').readlines()\n",
    "\n",
    "# conf_l_matrix_whole = []\n",
    "# conf_r_matrix_whole = []\n",
    "# conf_l_matrix_whole = open('conf_l_matrix.txt').readlines()\n",
    "# conf_r_matrix_whole = open('conf_r_matrix.txt').readlines()\n",
    "conf_matrix = []\n",
    "for i in range(0,len(conf_l_matrix_whole)):\n",
    "    conf_matrix.append([int(conf_r_matrix_whole[i].split(' ')[1]),int(conf_r_matrix_whole[i].split(' ')[2]), \\\n",
    "                        int(conf_l_matrix_whole[i].split(' ')[3]),int(conf_l_matrix_whole[i].split(' ')[4])])\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "im = ax.imshow(np.array(conf_matrix).T, interpolation='nearest', cmap=plt.cm.jet)\n",
    "fig.colorbar(im, ax=ax)\n",
    "plt.xlabel('Caffemodel Index(n)')\n",
    "plt.ylabel('like(like,dislike), dislike(like,dislike)')\n",
    "plt.title('Confusion Matrix@(dislike,like)')\n",
    "ax.set_aspect(25)\n",
    "fig.set_size_inches(20.5, 12.5)\n",
    "fig.savefig(recordDir+'cMat.png',bbox_inches='tight')\n",
    "# fig.savefig('cMat.png',bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save 2 caffemodels for user_specific fine tuning\n",
    "recordDir = '/local2/home/tong/fashionRecommendation/models/fashionNet_2/training_record/'\n",
    "\n",
    "# 1) [90000,110000]\n",
    "# caffemodel_idx:\n",
    "ndcg_whole = []\n",
    "ndcg_whole = open(recordDir+'t2.1_rank/ndcg.txt').readlines()\n",
    "ndcg_idx = []\n",
    "mean_ndcg = []\n",
    "ndcg_10 = []\n",
    "for i in range(0,len(ndcg_whole)):\n",
    "    if (i % 11 == 0):\n",
    "        ndcg_idx_temp = int(ndcg_whole[i+0].strip('\\r\\n').split(' ')[0])\n",
    "        if (ndcg_idx_temp>=90000 and ndcg_idx_temp<=110000):\n",
    "            ndcg_idx.append(ndcg_idx_temp)\n",
    "            mean_ndcg.append(float(ndcg_whole[i+0].strip('\\r\\n').split(' ')[1]))\n",
    "            ndcg_10.append(float(ndcg_whole[i+10].strip('\\r\\n').split(' ')[1]))\n",
    "print(\"H_mean_ndcg@({:.3f},{}), C_ndcg_10@({:.3f})\".format(max(mean_ndcg),ndcg_idx[mean_ndcg.index(max(mean_ndcg))],ndcg_10[mean_ndcg.index(max(mean_ndcg))]))\n",
    "\n",
    "test_accu_whole = []\n",
    "test_loss_whole = []\n",
    "test_accu_whole = open(recordDir+'test_rank_accu.txt').readlines()\n",
    "test_loss_whole = open(recordDir+'test_rank_loss.txt').readlines()\n",
    "test_iter_idx = []\n",
    "test_loss = []\n",
    "test_accuracy = []\n",
    "for i in range(0, len(test_accu_whole)):\n",
    "    #test_iter_idx\n",
    "    test_iter_idx.append(int(test_accu_whole[i].strip('\\r\\n').split(' ')[0]))\n",
    "    #test_loss\n",
    "    test_loss.append(float(test_loss_whole[i].strip('\\r\\n').split(' ')[1]))\n",
    "    #test_accuracy\n",
    "    test_accuracy.append(float(test_accu_whole[i].strip('\\r\\n').split(' ')[1]))\n",
    "test_idx_temp = test_iter_idx.index(ndcg_idx[mean_ndcg.index(max(mean_ndcg))])\n",
    "print(\"test_accu: {}, test_loss: {}\".format(test_accuracy[test_idx_temp], test_loss[test_idx_temp]))\n",
    "\n",
    "# 2) [110000,137706］\n",
    "# caffemodel_idx:\n",
    "ndcg_idx = []\n",
    "mean_ndcg = []\n",
    "ndcg_10 = []\n",
    "for i in range(0,len(ndcg_whole)):\n",
    "    if (i % 11 == 0):\n",
    "        ndcg_idx_temp = int(ndcg_whole[i+0].strip('\\r\\n').split(' ')[0])\n",
    "        if (ndcg_idx_temp>=110000 and ndcg_idx_temp<=137706):\n",
    "            ndcg_idx.append(ndcg_idx_temp)\n",
    "            mean_ndcg.append(float(ndcg_whole[i+0].strip('\\r\\n').split(' ')[1]))\n",
    "            ndcg_10.append(float(ndcg_whole[i+10].strip('\\r\\n').split(' ')[1]))\n",
    "print(\"H_mean_ndcg@({:.3f},{}), C_ndcg_10@({:.3f})\".format(max(mean_ndcg),ndcg_idx[mean_ndcg.index(max(mean_ndcg))],ndcg_10[mean_ndcg.index(max(mean_ndcg))]))\n",
    "\n",
    "test_accu_whole = []\n",
    "test_loss_whole = []\n",
    "test_accu_whole = open(recordDir+'test_rank_accu.txt').readlines()\n",
    "test_loss_whole = open(recordDir+'test_rank_loss.txt').readlines()\n",
    "test_iter_idx = []\n",
    "test_loss = []\n",
    "test_accuracy = []\n",
    "for i in range(0, len(test_accu_whole)):\n",
    "    #test_iter_idx\n",
    "    test_iter_idx.append(int(test_accu_whole[i].strip('\\r\\n').split(' ')[0]))\n",
    "    #test_loss\n",
    "    test_loss.append(float(test_loss_whole[i].strip('\\r\\n').split(' ')[1]))\n",
    "    #test_accuracy\n",
    "    test_accuracy.append(float(test_accu_whole[i].strip('\\r\\n').split(' ')[1]))  \n",
    "test_idx_temp = test_iter_idx.index(ndcg_idx[mean_ndcg.index(max(mean_ndcg))])\n",
    "print(\"test_accu: {}, test_loss: {}\".format(test_accuracy[test_idx_temp], test_loss[test_idx_temp]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
